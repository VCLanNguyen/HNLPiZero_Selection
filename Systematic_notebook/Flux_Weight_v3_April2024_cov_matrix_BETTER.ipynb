{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the relevant scripts from LArMachineLearningData\n",
    "# Nice the process so it can run with lots of cores on low priority\n",
    "import os\n",
    "os.nice(20)\n",
    "\n",
    "# Add local paths\n",
    "import sys\n",
    "hnlDIR = os.environ['_']\n",
    "sys.path.append('../pyscript')\n",
    "\n",
    "# From pyscript Library\n",
    "from Plotting import *\n",
    "from Dictionary import *\n",
    "from HelperFunctions import *\n",
    "from CutFunctions import *\n",
    "from SystematicsHelpers import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Configuration Stuff Here</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 200\n",
    "\n",
    "savePath = \"../plot_files/06April2024_m\"+str(m)+\"_v3_systematics/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>HNL</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../pkl_files/v3_April2024/df_hnl_m\"+str(m)+\"_v3_weight.pkl\", 'rb')\n",
    "df_hnl = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TPC Neutrino</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../pkl_files/v3_April2024/df_nu_m\"+str(m)+\"_v3_weight.pkl\", 'rb')\n",
    "df_nu = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intime Cosmics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../pkl_files/v3_April2024/df_cos_m\"+str(m)+\"_v3_weight.pkl\", 'rb')\n",
    "df_cos = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "#should be empty\n",
    "print(df_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hnl = df_hnl.reset_index()\n",
    "df_nu = df_nu.reset_index()\n",
    "df_cos = df_cos.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Organise Some Stuff</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnl_dict = {}\n",
    "rockbox_dict = {}\n",
    "ncpi0_dict = {}\n",
    "cos_dict = {}\n",
    "\n",
    "cos_error_list = ['stat']\n",
    "hnl_error_list = ['stat', 'flx']\n",
    "nu_error_list = ['stat', 'flx', 'xsec', 'g4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Make Beam Bucket - Post PID</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_counts = 1\n",
    "start_counts = 1\n",
    "    \n",
    "if m == 200:\n",
    "    true_counts = 1481.1881420359975\n",
    "    start_counts = 1356.6892755185766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simU = df_hnl['simU'].unique()[0]\n",
    "plotU = df_hnl['scaledU'].unique()[0]\n",
    "scaleHNLPlot = (plotU/simU)**2\n",
    "\n",
    "true_counts = true_counts * scaleHNLPlot\n",
    "start_counts =  start_counts * scaleHNLPlot\n",
    "\n",
    "print(\"Scale Umu by \" + str(scaleHNLPlot))\n",
    "print(\"Scaled Umu = \" + str(plotU))\n",
    "print(\"Scaled true counts = \" + str(true_counts))\n",
    "print(\"Scaled start counts = \" + str(start_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnl_label = str(m) + ' MeV HNL ${\\pi}^{0}$' + '\\n' + '|U$_{{\\mu 4}}$|$^{{2}}$ = ' +str(sci_notation(simU,0,0))\n",
    "rockbox_label = \"Rockbox Neutrinos\"\n",
    "ncpi0_label = \"NCPi0\"\n",
    "cos_label = \"Cosmics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 18.936\n",
    "this_label = str(m) + ' MeV HNL ${\\pi}^{0}$' + '\\n' + '|U$_{{\\mu 4}}$|$^{{2}}$ = ' +str(sci_notation(plotU,0,0))\n",
    "\n",
    "hist, bins = plot_slc_var(df_hnl, df_nu, df_cos,\n",
    "                    true_counts, start_counts, \n",
    "                    'mod_t', \n",
    "                    xmin = 0, xmax = 30, xnbin = 30,\n",
    "                    xtitle = 'Opt0 Time Corrected Z % ' + str(width) + ' [ns]',\n",
    "                    ifAddLegend = True, addLegend = this_label\n",
    "                    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Separate Into Signal and Background Sample </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos_nu = df_nu[df_nu['slc_true_event_type'] == 9]\n",
    "df_cos_hnl = df_hnl[df_hnl['slc_true_event_type'] == 9]\n",
    "\n",
    "df_nu = df_nu[df_nu['slc_true_event_type'] != 9]\n",
    "df_hnl = df_hnl[df_hnl['slc_true_event_type'] != 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos = pd.concat([df_cos, df_cos_nu, df_cos_hnl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_cos_nu\n",
    "del df_cos_hnl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot Individual Sample</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (18,4))\n",
    "\n",
    "xmin, xmax, xnbin = xmin, xmax, xnbin\n",
    "xlimmin, xlimmax = xmin, xmax\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "pltdf = df_hnl['mod_t']\n",
    "weights = df_hnl['scale_pot']\n",
    "\n",
    "_, _, _ = ax1.hist(\n",
    "                            pltdf,\n",
    "                            bins = np.arange(xmin, xmax+(xmax-xmin)/xnbin, (xmax-xmin)/xnbin),\n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = hnl_col,\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = this_label\n",
    "                        )\n",
    "#-----------------------------------------------------------------#\n",
    "pltdf = df_nu['mod_t']\n",
    "weights = df_nu['scale_pot']\n",
    "\n",
    "_, _, _ = ax2.hist(\n",
    "                            pltdf,\n",
    "                            bins = np.arange(xmin, xmax+(xmax-xmin)/xnbin, (xmax-xmin)/xnbin),\n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = nu_col,\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = \"Neutrino\"\n",
    "                        )\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "pltdf = df_cos['mod_t']\n",
    "weights = df_cos['scale_pot']\n",
    "\n",
    "_, _, _ = ax3.hist(\n",
    "                            pltdf,\n",
    "                            bins = np.arange(xmin, xmax+(xmax-xmin)/xnbin, (xmax-xmin)/xnbin),\n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = col_dict['PastelGreen'],\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = \"Cosmics\"\n",
    "                        )\n",
    "#-----------------------------------------------------------------#\n",
    "ax1.legend(loc = 'upper left',fontsize = 12)\n",
    "plot_tick(ax1, 16)\n",
    "plot_title(ax1, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  r\"Slices (1$\\times10^{21}$ POT)\", 16)\n",
    "\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.set_ylim(1, 5000)\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "ax2.legend(loc = 'upper left',fontsize = 12)\n",
    "plot_tick(ax2, 16)\n",
    "plot_title(ax2, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  \"\", 16)\n",
    "\n",
    "ax2.set_xlim(xmin, xmax)\n",
    "ax2.set_ylim(1, 2000)\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "ax3.legend(loc = 'upper left',fontsize = 12)\n",
    "plot_tick(ax3, 16)\n",
    "plot_title(ax3, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  \"\", 16)\n",
    "\n",
    "ax3.set_xlim(xmin, xmax)\n",
    "ax3.set_ylim(1, 35)\n",
    "#-----------------------------------------------------------------#\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_neutrino_overlay.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>HNL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin it\n",
    "hnl_cv, _ = np.histogram(np.array(df_hnl['mod_t']), bins = np.arange(xmin, xmax+(xmax-xmin)/xnbin, (xmax-xmin)/xnbin))\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(hnl_cv)\n",
    "\n",
    "#Make covariance matrix\n",
    "hnl_stat_cov = np.diag(hnl_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "hnl_stat_err = np.sqrt(np.diag(hnl_stat_cov))\n",
    "\n",
    "print(\"\\n stat err\")\n",
    "print(hnl_stat_err)\n",
    "\n",
    "#save in dictionary\n",
    "hnl_cv_plot = np.insert(hnl_cv, 0, 0)\n",
    "\n",
    "bins_mid = np.convolve(bins, [0.5, 0.5], \"valid\")\n",
    "\n",
    "hnl_dict['cv'] = hnl_cv\n",
    "hnl_dict['cv_plot'] = hnl_cv_plot\n",
    "\n",
    "hnl_dict['stat_cov'] = hnl_stat_cov\n",
    "hnl_dict['stat_err'] = hnl_stat_err\n",
    "\n",
    "#plot and save\n",
    "plot_hatchy_hatch(hnl_dict, hnl_label, \"hnl\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_statistics_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Flux</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make covariance matrix per variable\n",
    "hnl_flx_cov_array =loopy_loop_multisim_universe(flux_list, flux_name, 1000, df_hnl, hnl_dict, 'hnl', savePath)\n",
    "\n",
    "#Add them together\n",
    "hnl_flx_cov = new_19_by_19_cov()\n",
    "\n",
    "for cov in hnl_flx_cov_array:\n",
    "    hnl_flx_cov = hnl_flx_cov + cov\n",
    "    \n",
    "hnl_flx_err = np.sqrt(np.diag(hnl_flx_cov))\n",
    "\n",
    "#save in dictionary\n",
    "hnl_dict['flx_cov'] = hnl_flx_cov\n",
    "hnl_dict['flx_err'] = hnl_flx_err\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(hnl_dict, hnl_label, \"hnl\", \"flx_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_flux_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combine Errors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(hnl_dict, hnl_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(hnl_dict, \"hnl\", hnl_label, hnl_error_list)\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_beam_bucket_combined_covariance.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale To POT AND Umu Coupling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnl_scale_factor = df_hnl['scale_pot'].unique()[0]\n",
    "fitU = plotU\n",
    "print(hnl_scale_factor)\n",
    "print(fitU)\n",
    "\n",
    "scale = 1/100\n",
    "hnl_scale_factor = hnl_scale_factor * scale\n",
    "fitU = getUfromScaleFactor(fitU, scale)\n",
    "print(hnl_scale_factor)\n",
    "print(fitU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cov_matrix(hnl_dict, hnl_scale_factor, hnl_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combine_err(hnl_dict, \"hnl\", hnl_label, hnl_error_list\n",
    "                  , ifScale = True , scaleYmax = hnl_scale_factor, suffix = '_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_beam_bucket_combined_covariance_scaled.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Separate Neutrinos sample</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pot = df_nu['scale_pot'].unique()\n",
    "print(unique_pot)\n",
    "\n",
    "df_nu_rockbox = df_nu[df_nu['scale_pot'] ==  max(unique_pot)]\n",
    "\n",
    "df_nu_ncpi0 = df_nu[df_nu['scale_pot'] ==  min(unique_pot)]\n",
    "\n",
    "print(len(df_nu))\n",
    "print(len(df_nu_rockbox))\n",
    "print(len(df_nu_ncpi0))\n",
    "\n",
    "print(df_nu_rockbox['scale_pot'].unique())\n",
    "print(df_nu_ncpi0['scale_pot'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Rockbox Neutrino</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin it\n",
    "rockbox_cv, _ = np.histogram(np.array(df_nu_rockbox['mod_t']), bins = np.arange(xmin, xmax+(xmax-xmin)/xnbin, (xmax-xmin)/xnbin))\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(rockbox_cv)\n",
    "\n",
    "#make covariance matrix\n",
    "rockbox_stat_cov = np.diag(rockbox_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "rockbox_stat_err = np.sqrt(np.diag(rockbox_stat_cov))\n",
    "\n",
    "print(\"\\n stat err\")\n",
    "print(rockbox_stat_err)\n",
    "\n",
    "#save in dictionary\n",
    "rockbox_cv_plot = np.insert(rockbox_cv, 0, 0)\n",
    "\n",
    "rockbox_dict['cv'] = rockbox_cv\n",
    "rockbox_dict['cv_plot'] = rockbox_cv_plot\n",
    "\n",
    "rockbox_dict['stat_cov'] = rockbox_stat_cov\n",
    "rockbox_dict['stat_err'] = rockbox_stat_err\n",
    "\n",
    "#plot and save\n",
    "plot_hatchy_hatch(rockbox_dict, rockbox_label, \"rockbox\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_stats_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Flux</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_flx_cov_array =loopy_loop_multisim_universe(flux_list, flux_name, 1000\n",
    "                                                    , df_nu_rockbox, rockbox_dict, 'rockbox'\n",
    "                                                    , savePath)\n",
    "\n",
    "#Add them together\n",
    "rockbox_flx_cov = new_19_by_19_cov()\n",
    "\n",
    "for cov in rockbox_flx_cov_array:\n",
    "    rockbox_flx_cov = rockbox_flx_cov + cov\n",
    "    \n",
    "rockbox_flx_err = np.sqrt(np.diag(rockbox_flx_cov))\n",
    "\n",
    "rockbox_dict['flx_cov'] = rockbox_flx_cov\n",
    "rockbox_dict['flx_err'] = rockbox_flx_err\n",
    "\n",
    "#plot it\n",
    "\n",
    "plot_hatchy_hatch(rockbox_dict, rockbox_label, \"rockbox\", \"flx_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_flux_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: UniSim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in unisim_list:\n",
    "    df_nu_rockbox[name] = df_nu_rockbox[name].apply(lambda row: check_unisim(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_unisim_cov_array = loopy_loop_unisim(df_nu_rockbox, rockbox_dict, \"rockbox\", savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sigma</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a random gaussian gun\n",
    "mu, sigma = 0, 1 # mean and standard deviation\n",
    "n_univ = len(df_nu['slc_xsec_multisim_total'][0])\n",
    "\n",
    "random_arr = np.random.normal(mu, sigma, n_univ)\n",
    "len_univ = np.arange(0, n_univ)\n",
    "plt.hist(random_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_multisigma_cov_arr = loopy_loop_multisigma(multisigma_list, multisigma_list, random_arr\n",
    "                                                   , df_nu_rockbox, rockbox_dict, \"rockbox\"\n",
    "                                                   , savePath\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_multisim_cov_array =loopy_loop_multisim_universe(multisim_list, multisim_list, 500\n",
    "                                                         , df_nu_rockbox, rockbox_dict, 'rockbox'\n",
    "                                                         , savePath\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Combined</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rockbox_unisim_cov_array))\n",
    "print(len(rockbox_multisigma_cov_arr))\n",
    "print(len(rockbox_multisim_cov_array))\n",
    "\n",
    "rockbox_xsec_cov_array = rockbox_unisim_cov_array + rockbox_multisigma_cov_arr + rockbox_multisim_cov_array\n",
    "print(len(rockbox_xsec_cov_array))\n",
    "\n",
    "rockbox_xsec_cov = new_19_by_19_cov()\n",
    "\n",
    "for cov in rockbox_xsec_cov_array:\n",
    "    rockbox_xsec_cov = rockbox_xsec_cov + cov\n",
    "    \n",
    "rockbox_xsec_err = np.sqrt(np.diag(rockbox_xsec_cov))\n",
    "\n",
    "rockbox_dict['xsec_cov'] = rockbox_xsec_cov\n",
    "rockbox_dict['xsec_err'] = rockbox_xsec_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hatchy_hatch(rockbox_dict, rockbox_label, \"rockbox\", \"xsec_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_xsec_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Geant4 Re-Interactions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_g4_cov = loopy_loop_multisim_universe(g4_list, g4_name, 1000, df_nu_rockbox, rockbox_dict, \"rockbox\", savePath)\n",
    "rockbox_g4_cov = rockbox_g4_cov[0]\n",
    "\n",
    "rockbox_g4_err = np.sqrt(np.diag(rockbox_g4_cov))\n",
    "\n",
    "#save it\n",
    "rockbox_dict['g4_cov'] = rockbox_g4_cov\n",
    "rockbox_dict['g4_err'] = rockbox_g4_err\n",
    "print(rockbox_g4_err)\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(rockbox_dict, rockbox_label, \"rockbox\", \"g4_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_g4_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Combine Errors </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(rockbox_dict, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(rockbox_dict, \"rockbox\", rockbox_label, nu_error_list)\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_beam_bucket_combined_covariance.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Scale to POT </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_scale_factor = df_nu_rockbox['scale_pot'].unique()[0]\n",
    "print(rockbox_scale_factor)\n",
    "\n",
    "scale_cov_matrix(rockbox_dict, rockbox_scale_factor, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(rockbox_dict, \"rockbox\", rockbox_label, nu_error_list\n",
    "                  , ifScale = True , scaleYmax = rockbox_scale_factor, suffix ='_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_beam_bucket_combined_covariance_scaked.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NCPi0</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin it\n",
    "ncpi0_cv, _ = np.histogram(np.array(df_nu_ncpi0['mod_t']), bins = np.arange(xmin, xmax+(xmax-xmin)/xnbin, (xmax-xmin)/xnbin))\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(ncpi0_cv)\n",
    "\n",
    "#This is the smart way\n",
    "ncpi0_stat_cov = np.diag(ncpi0_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "ncpi0_stat_err = np.sqrt(np.diag(ncpi0_stat_cov))\n",
    "\n",
    "print(\"\\n stat err\")\n",
    "print(ncpi0_stat_err)\n",
    "\n",
    "#save in dictionary\n",
    "ncpi0_cv_plot = np.insert(ncpi0_cv, 0, 0)\n",
    "\n",
    "ncpi0_dict['cv'] = ncpi0_cv\n",
    "ncpi0_dict['cv_plot'] = ncpi0_cv_plot\n",
    "\n",
    "ncpi0_dict['stat_cov'] = ncpi0_stat_cov\n",
    "ncpi0_dict['stat_err'] = ncpi0_stat_err\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(ncpi0_dict, ncpi0_label, \"ncpi0\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_stat_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Flux</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_flx_cov_array =loopy_loop_multisim_universe(flux_list, flux_name, 1000, df_nu_ncpi0, ncpi0_dict, 'ncpi0', savePath)\n",
    "\n",
    "#add cov matrix\n",
    "ncpi0_flx_cov = new_19_by_19_cov()\n",
    "\n",
    "for cov in ncpi0_flx_cov_array:\n",
    "    ncpi0_flx_cov = ncpi0_flx_cov + cov\n",
    "    \n",
    "ncpi0_flx_err = np.sqrt(np.diag(ncpi0_flx_cov))\n",
    "\n",
    "ncpi0_dict['flx_cov'] = ncpi0_flx_cov\n",
    "ncpi0_dict['flx_err'] = ncpi0_flx_err\n",
    "\n",
    "#plot it\n",
    "\n",
    "plot_hatchy_hatch(ncpi0_dict, ncpi0_label, \"ncpi0\", \"flx_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_flx_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: UniSim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in unisim_list:\n",
    "    \n",
    "    df_nu_ncpi0[name] = df_nu_ncpi0[name].apply(lambda row: check_unisim(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_unisim_cov_array = loopy_loop_unisim(df_nu_ncpi0, ncpi0_dict, \"ncpi0\", savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sigma</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_multisigma_cov_arr = loopy_loop_multisigma(multisigma_list, multisigma_list, random_arr\n",
    "                                                 , df_nu_ncpi0, ncpi0_dict, \"ncpi0\"\n",
    "                                                 , savePath\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_multisim_cov_array =loopy_loop_multisim_universe(multisim_list, multisim_list, 500\n",
    "                                                       , df_nu_ncpi0, ncpi0_dict, 'ncpi0'\n",
    "                                                       , savePath\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Combined</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ncpi0_unisim_cov_array))\n",
    "print(len(ncpi0_multisigma_cov_arr))\n",
    "print(len(ncpi0_multisim_cov_array))\n",
    "\n",
    "ncpi0_xsec_cov_array = ncpi0_unisim_cov_array  + ncpi0_multisigma_cov_arr + ncpi0_multisim_cov_array\n",
    "print(len(ncpi0_xsec_cov_array))\n",
    "\n",
    "ncpi0_xsec_cov = new_19_by_19_cov()\n",
    "\n",
    "for cov in ncpi0_xsec_cov_array:\n",
    "    ncpi0_xsec_cov = ncpi0_xsec_cov + cov\n",
    "    \n",
    "ncpi0_xsec_err = np.sqrt(np.diag(ncpi0_xsec_cov))\n",
    "\n",
    "ncpi0_dict['xsec_cov'] = ncpi0_xsec_cov\n",
    "ncpi0_dict['xsec_err'] = ncpi0_xsec_err\n",
    "\n",
    "print(ncpi0_xsec_err)\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(ncpi0_dict, ncpi0_label, \"ncpi0\", \"xsec_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_xsec_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Geant4 Re-Interactions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_g4_cov = loopy_loop_multisim_universe(g4_list, g4_name, 1000, df_nu_ncpi0, ncpi0_dict, \"ncpi0\", savePath)\n",
    "ncpi0_g4_cov = ncpi0_g4_cov[0]\n",
    "\n",
    "ncpi0_g4_err = np.sqrt(np.diag(ncpi0_g4_cov))\n",
    "\n",
    "ncpi0_dict['g4_cov'] = ncpi0_g4_cov\n",
    "ncpi0_dict['g4_err'] = ncpi0_g4_err\n",
    "print(ncpi0_g4_err)\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(ncpi0_dict, ncpi0_label, \"ncpi0\", \"g4_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_g4_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combine Errors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(ncpi0_dict, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(ncpi0_dict, \"ncpi0\", ncpi0_label, nu_error_list)\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_beam_bucket_combined_covariance.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale to POT </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_scale_factor = df_nu_ncpi0['scale_pot'].unique()[0]\n",
    "print(ncpi0_scale_factor)\n",
    "\n",
    "scale_cov_matrix(ncpi0_dict, ncpi0_scale_factor, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(ncpi0_dict, \"ncpi0\", ncpi0_label, nu_error_list\n",
    "                 , ifScale = True, scaleYmax = ncpi0_scale_factor, suffix ='_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_beam_bucket_combined_covariance_scaked.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fuse Neutrino Background Into One Please</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_dict = {}\n",
    "nu_dict['cv_scale'] = rockbox_dict['cv_scale'] + ncpi0_dict['cv_scale']\n",
    "nu_dict['cv_plot_scale'] = rockbox_dict['cv_plot_scale'] + ncpi0_dict['cv_plot_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_nan_plz(rockbox_dict)\n",
    "fill_nan_plz(ncpi0_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_dict['combined_cov_scale'] = new_19_by_19_cov()\n",
    "\n",
    "for err in nu_error_list:\n",
    "    print(err)\n",
    "    nu_dict[err+'_cov_scale'] = rockbox_dict[err+'_cov_scale'] + ncpi0_dict[err+'_cov_scale']\n",
    "    nu_dict['combined_cov_scale'] = nu_dict['combined_cov_scale'] + nu_dict[err+'_cov_scale']\n",
    "    \n",
    "    nu_dict[err+'_err_scale'] = np.sqrt(np.diag(nu_dict[err+'_cov_scale']))\n",
    "    \n",
    "    nu_dict[err+'_cov_frac_scale'] = nu_dict[err+'_cov_scale'] / np.outer(nu_dict['cv_scale'], nu_dict['cv_scale'])\n",
    "    nu_dict[err+'_frac_err_scale'] = np.sqrt(np.diag(nu_dict[err+'_cov_frac_scale']))\n",
    "    \n",
    "nu_dict['combined_err_scale'] = np.sqrt(np.diag( nu_dict['combined_cov_scale']))\n",
    "    \n",
    "nu_dict['combined_cov_frac_scale'] = nu_dict['combined_cov_scale'] / np.outer(nu_dict['cv_scale'], nu_dict['cv_scale'])\n",
    "nu_dict['combined_frac_err_scale'] = np.sqrt(np.diag(nu_dict['combined_cov_frac_scale']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combine_err(nu_dict, \"nu\", \"Neutrinos\", nu_error_list\n",
    "                  , ifScale = True, suffix = '_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"combined_neutrino_beam_bucket_combined_covariance_scaled.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Cosmics </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Separate Into Each Sample</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cos_pot = df_cos['scale_pot'].unique()\n",
    "print(unique_cos_pot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos_rockbox = df_cos[df_cos['scale_pot'] == max(unique_cos_pot)]\n",
    "df_cos_hnl = df_cos[df_cos['scale_pot'] == min(unique_cos_pot)]\n",
    "\n",
    "cos_rockbox_dict = {}\n",
    "cos_hnl_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rockbox Cosmics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin it\n",
    "cos_rockbox_cv, _ = np.histogram(np.array(df_cos_rockbox['mod_t']), bins = np.arange(xmin, xmax+(xmax-xmin)/xnbin, (xmax-xmin)/xnbin))\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(cos_rockbox_cv)\n",
    "\n",
    "#This is the smart way\n",
    "cos_rockbox_stat_cov = np.diag(cos_rockbox_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "cos_rockbox_stat_err = np.sqrt(np.diag(cos_rockbox_stat_cov))\n",
    "\n",
    "print(\"\\n stat err\")\n",
    "print(cos_rockbox_stat_err)\n",
    "\n",
    "#save in dictionary\n",
    "cos_rockbox_cv_plot = np.insert(cos_rockbox_cv, 0, 0)\n",
    "\n",
    "cos_rockbox_dict['cv'] = cos_rockbox_cv\n",
    "cos_rockbox_dict['cv_plot'] = cos_rockbox_cv_plot\n",
    "\n",
    "cos_rockbox_dict['stat_cov'] = cos_rockbox_stat_cov\n",
    "cos_rockbox_dict['stat_err'] = cos_rockbox_stat_err\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(cos_rockbox_dict, \"Rockbox Cosmics\", \"cos\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"cosmic_rockbox_stat_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>HNL Cosmics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin it\n",
    "cos_hnl_cv, _ = np.histogram(np.array(df_cos_hnl['mod_t']), bins = np.arange(xmin, xmax+(xmax-xmin)/xnbin, (xmax-xmin)/xnbin))\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(cos_hnl_cv)\n",
    "\n",
    "#This is the smart way\n",
    "cos_hnl_stat_cov = np.diag(cos_hnl_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "cos_hnl_stat_err = np.sqrt(np.diag(cos_hnl_stat_cov))\n",
    "\n",
    "print(\"\\n stat err\")\n",
    "print(cos_hnl_stat_err)\n",
    "\n",
    "#save in dictionary\n",
    "cos_hnl_cv_plot = np.insert(cos_hnl_cv, 0, 0)\n",
    "\n",
    "cos_hnl_dict['cv'] = cos_hnl_cv\n",
    "cos_hnl_dict['cv_plot'] = cos_hnl_cv_plot\n",
    "\n",
    "cos_hnl_dict['stat_cov'] = cos_hnl_stat_cov\n",
    "cos_hnl_dict['stat_err'] = cos_hnl_stat_err\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(cos_hnl_dict, \"HNL Cosmics\", \"cos\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"cosmic_hnl_stat_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale and Combine Cosmics </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(cos_rockbox_dict, cos_error_list)\n",
    "\n",
    "combine_error(cos_hnl_dict, cos_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cov_matrix(cos_rockbox_dict, rockbox_scale_factor, cos_error_list)\n",
    "\n",
    "scale_cov_matrix(cos_hnl_dict, hnl_scale_factor, cos_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_rockbox_dict['cv_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_nan_plz(cos_rockbox_dict)\n",
    "fill_nan_plz(cos_hnl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dict = {}\n",
    "cos_dict['cv_scale'] = cos_rockbox_dict['cv_scale'] + cos_hnl_dict['cv_scale']\n",
    "cos_dict['cv_plot_scale'] = cos_rockbox_dict['cv_plot_scale'] + cos_hnl_dict['cv_plot_scale']\n",
    "\n",
    "cos_dict['stat_cov_scale'] = cos_rockbox_dict['stat_cov_scale'] + cos_hnl_dict['stat_cov_scale']\n",
    "cos_dict['stat_err_scale'] = np.sqrt(np.diag(cos_dict['stat_cov_scale']))\n",
    "\n",
    "cos_dict['stat_cov_frac_scale'] = cos_dict['stat_cov_scale'] / np.outer(cos_dict['cv_scale'], cos_dict['cv_scale'])\n",
    "cos_dict['stat_frac_err_scale'] = np.sqrt(np.diag(cos_dict['stat_cov_frac_scale']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combine_err(cos_dict, \"cos\", cos_label, cos_error_list, ifScale = True, scaleYmax = 10, suffix = '_scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fuse Neutrino and Cosmics Into A Single Background</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_nan_plz(cos_dict)\n",
    "fill_nan_plz(nu_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_dict = {}\n",
    "\n",
    "#combine cv\n",
    "bkg_dict['cv_scale'] = cos_dict['cv_scale'] + nu_dict['cv_scale']\n",
    "bkg_dict['cv_plot_scale'] = cos_dict['cv_plot_scale'] + nu_dict['cv_plot_scale']\n",
    "\n",
    "#combine stat errors\n",
    "bkg_dict['stat_cov_scale'] = cos_dict['stat_cov_scale'] + nu_dict['stat_cov_scale']\n",
    "\n",
    "bkg_dict['stat_err_scale'] = np.sqrt(np.diag(bkg_dict['stat_cov_scale']))\n",
    "\n",
    "bkg_dict['stat_cov_frac_scale'] = bkg_dict['stat_cov_scale'] / np.outer(bkg_dict['cv_scale'], bkg_dict['cv_scale'])\n",
    "bkg_dict['stat_frac_err_scale'] = np.sqrt(np.diag(bkg_dict['stat_cov_frac_scale']))\n",
    "\n",
    "#flux / xsec / g4 are taken directly from neutrino samples\n",
    "bkg_dict['flx_frac_err_scale'] = nu_dict['flx_frac_err_scale']\n",
    "bkg_dict['xsec_frac_err_scale'] = nu_dict['xsec_frac_err_scale']\n",
    "bkg_dict['g4_frac_err_scale'] = nu_dict['g4_frac_err_scale']\n",
    "\n",
    "bkg_dict['flx_cov_scale'] = nu_dict['flx_cov_scale']\n",
    "bkg_dict['xsec_cov_scale'] = nu_dict['xsec_cov_scale']\n",
    "bkg_dict['g4_cov_scale'] = nu_dict['g4_cov_scale']\n",
    "\n",
    "#combine covariance matrix into one AFTER scaling\n",
    "bkg_dict['combined_cov_scale'] = new_19_by_19_cov()\n",
    "\n",
    "for err in nu_error_list:\n",
    "    bkg_dict['combined_cov_scale'] = bkg_dict['combined_cov_scale'] + bkg_dict[err+'_cov_scale']\n",
    "\n",
    "bkg_dict['combined_err_scale'] = np.sqrt(np.diag( bkg_dict['combined_cov_scale']))\n",
    "    \n",
    "bkg_dict['combined_cov_frac_scale'] = bkg_dict['combined_cov_scale'] / np.outer(bkg_dict['cv_scale'], bkg_dict['cv_scale'])\n",
    "bkg_dict['combined_frac_err_scale'] = np.sqrt(np.diag(bkg_dict['combined_cov_frac_scale']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_dict['combined_frac_err_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combine_err(bkg_dict, \"nu\", \"Neutrino + Cosmic Background\"\n",
    "                 , nu_error_list\n",
    "                 , ifScale = True, scaleYmax = 1.2, suffix = '_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnl_dict['m'] = 200\n",
    "hnl_dict['fitU'] = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../pkl_files/v3_April2024/hnl_m\"+str(m)+\"_v3_weight.npy\", hnl_dict) \n",
    "np.save(\"../pkl_files/v3_April2024/rock_m\"+str(m)+\"_v3_weight.npy\", rockbox_dict)\n",
    "np.save(\"../pkl_files/v3_April2024/ncpi0_m\"+str(m)+\"_v3_weight.npy\", ncpi0_dict) \n",
    "np.save(\"../pkl_files/v3_April2024/nu_m\"+str(m)+\"_v3_weight.npy\", nu_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
