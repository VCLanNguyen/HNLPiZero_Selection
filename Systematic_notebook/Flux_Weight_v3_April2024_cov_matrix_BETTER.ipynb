{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the relevant scripts from LArMachineLearningData\n",
    "# Nice the process so it can run with lots of cores on low priority\n",
    "import os\n",
    "os.nice(20)\n",
    "\n",
    "# Add local paths\n",
    "import sys\n",
    "hnlDIR = os.environ['_']\n",
    "sys.path.append('../pyscript')\n",
    "\n",
    "# From pyscript Library\n",
    "from Plotting import *\n",
    "from Dictionary import *\n",
    "from HelperFunctions import *\n",
    "from CutFunctions import *\n",
    "from SystematicsHelpers import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Configuration Stuff Here</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 200\n",
    "\n",
    "savePath = \"../plot_files/14April2024_m\"+str(m)+\"_v3_systematics/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>HNL</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../pkl_files/v3_April2024/df_postselect_weight_m\"+str(m)+\"_v3_hnl.pkl\", 'rb')\n",
    "df_hnl = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TPC Neutrino</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../pkl_files/v3_April2024/df_postselect_weight_v3_nu.pkl\", 'rb')\n",
    "df_nu = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intime Cosmics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../pkl_files/v3_April2024/df_postselect_weight_v3_cos.pkl\", 'rb')\n",
    "df_cos = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "#should be empty\n",
    "print(df_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hnl = df_hnl.reset_index()\n",
    "df_nu = df_nu.reset_index()\n",
    "df_cos = df_cos.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Do HNL Scaling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_counts = df_hnl['true_counts'][0] \n",
    "start_counts = df_hnl['true_counts'][0]\n",
    "\n",
    "simU = df_hnl['simU'].unique()[0]\n",
    "plotU = df_hnl['plotU'].unique()[0]\n",
    "\n",
    "scale = (plotU/simU)**2\n",
    "\n",
    "hnl_scale_factor = df_hnl['scale_pot'].unique()[0]\n",
    "sim_hnl_scale_factor = df_hnl['sim_scale_pot'].unique()[0]\n",
    "\n",
    "print(\"Sim U = {}\".format(simU))\n",
    "print(\"Sim POT scale factor = {}\".format(sim_hnl_scale_factor))\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Sim to Plot scale by = {}\".format(scale))\n",
    "print(\"Plot U = {}\".format(plotU))\n",
    "print(\"Plot POT scale factor = {}\".format(hnl_scale_factor))\n",
    "\n",
    "\n",
    "scale = 1/100\n",
    "hnl_scale_factor = hnl_scale_factor * scale\n",
    "fitU = getUfromScaleFactor(plotU, scale)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Sim to Plot scale by = {}\".format(scale))\n",
    "print(\"Fit U = {}\".format(fitU))\n",
    "print(\"Fit POT scale factor = {}\".format(hnl_scale_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Separate Cosmics Sample </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos_nu = df_nu[df_nu['slc_true_event_type'] == 9]\n",
    "df_cos_hnl = df_hnl[df_hnl['slc_true_event_type'] == 9]\n",
    "\n",
    "df_nu = df_nu[df_nu['slc_true_event_type'] != 9]\n",
    "df_hnl = df_hnl[df_hnl['slc_true_event_type'] != 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep_branches = [ \"run\", \"subrun\", \"event\", \"slc_id\", \"slc_comp\", \"scale_pot\", \"mod_t\"]\n",
    "keep_branches = [ \"scale_pot\", \"mod_t\"]\n",
    "\n",
    "df_cos_nu = df_cos_nu[keep_branches]\n",
    "df_cos_hnl = df_cos_hnl[keep_branches]\n",
    "\n",
    "print(\"Cosmics #events from Rockbox = {}\".format(len(df_cos_nu)))\n",
    "print(\"Cosmics #events from HNL = {}\".format(len(df_cos_hnl)))\n",
    "\n",
    "df_cos = pd.concat([df_cos_nu])#, df_cos_hnl])\n",
    "print(\"Cosmics #events = {}\".format(len(df_cos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rockbox Cosmics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "even_bins_mid = np.convolve(even_bins, [0.5, 0.5], \"valid\")\n",
    "\n",
    "cos_true =[]\n",
    "cos_true, _ = np.histogram(np.array(df_cos['mod_t']), bins=even_bins)\n",
    "\n",
    "print(cos_true)\n",
    "print(cos_true.sum())\n",
    "\n",
    "#fill bins intergral = number of bins \n",
    "cos_flat = np.array([len(df_cos)/ 19] * 19)\n",
    "print(cos_flat)\n",
    "print(cos_flat.sum())\n",
    "\n",
    "cos_flat_rebin = []\n",
    "\n",
    "#rebin flat distribution into the chosen bins\n",
    "for first, second in zip(bins, bins[1:]):\n",
    "    #print(first, second)\n",
    "    #print(cos_norm_cv[first:second])\n",
    "    cos_flat_rebin.append(cos_flat[first:second].sum())   \n",
    "    \n",
    "cos_flat_rebin = np.array(cos_flat_rebin)\n",
    "print(cos_flat_rebin)\n",
    "print(cos_flat_rebin.sum())\n",
    "\n",
    "#=================================================================#\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (6,4))\n",
    "#-----------------------------------------------------------------#\n",
    "ax.step(even_bins, np.insert(cos_true, 0, 0)\n",
    "         , color = cos_col\n",
    "         , label =  \"True Cosmics\"\n",
    ")\n",
    "#-----------------------------------------------------------------#\n",
    "ax.step(even_bins, np.insert(cos_flat, 0, 0)\n",
    "         , color = col_dict[\"Peach\"]\n",
    "         , label =  \"Flat Cosmics\"\n",
    ")\n",
    "#-----------------------------------------------------------------#\n",
    "ax.step(bins, np.insert(cos_flat_rebin, 0, 0)\n",
    "         , color = col_dict[\"Teal\"]\n",
    "         , label =  \"Rebin Flat Cosmics\"\n",
    ")\n",
    "#-----------------------------------------------------------------#\n",
    "ax.legend(loc = 'upper left',fontsize = 14)\n",
    "\n",
    "plot_tick(ax, 16)\n",
    "plot_title(ax, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  \"Slices (No Scaling)\", 16)\n",
    "\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(0, 2)\n",
    "#-----------------------------------------------------------------#\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(savePath+\"_flatten_and_rebin_cosmics\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_true_hnl =[]\n",
    "cos_true_hnl, _ = np.histogram(np.array(df_cos_hnl['mod_t']), bins=even_bins)\n",
    "\n",
    "print(cos_true_hnl)\n",
    "print(cos_true_hnl.sum())\n",
    "\n",
    "#fill bins intergral = number of bins \n",
    "cos_flat_hnl = np.array([len(df_cos_hnl)/ 19] * 19)\n",
    "print(cos_flat_hnl)\n",
    "print(cos_flat_hnl.sum())\n",
    "\n",
    "cos_flat_rebin_hnl = []\n",
    "\n",
    "#rebin flat distribution into the chosen bins\n",
    "for first, second in zip(bins, bins[1:]):\n",
    "    cos_flat_rebin_hnl.append(cos_flat_hnl[first:second].sum())   \n",
    "    \n",
    "cos_flat_rebin_hnl = np.array(cos_flat_rebin_hnl)\n",
    "print(cos_flat_rebin_hnl)\n",
    "print(cos_flat_rebin_hnl.sum())\n",
    "\n",
    "#=================================================================#\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (6,4))\n",
    "#-----------------------------------------------------------------#\n",
    "ax.step(even_bins, np.insert(cos_true_hnl, 0, 0)\n",
    "         , color = cos_col\n",
    "         , label =  \"True Cosmics\"\n",
    ")\n",
    "#-----------------------------------------------------------------#\n",
    "ax.step(even_bins, np.insert(cos_flat_hnl, 0, 0)\n",
    "         , color = col_dict[\"Peach\"]\n",
    "         , label =  \"Flat Cosmics\"\n",
    ")\n",
    "#-----------------------------------------------------------------#\n",
    "ax.step(bins, np.insert(cos_flat_rebin_hnl, 0, 0)\n",
    "         , color = col_dict[\"Teal\"]\n",
    "         , label =  \"Rebin Flat Cosmics\"\n",
    ")\n",
    "#-----------------------------------------------------------------#\n",
    "ax.legend(loc = 'upper left',fontsize = 14)\n",
    "\n",
    "plot_tick(ax, 16)\n",
    "plot_title(ax, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  \"Slices (No Scaling)\", 16)\n",
    "\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(0, 10)\n",
    "#-----------------------------------------------------------------#\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(savePath+\"_flatten_and_rebin_HNL_cosmics\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Separate Neutrinos sample</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pot = df_nu['scale_pot'].unique()\n",
    "\n",
    "rockbox_scale_factor = 1\n",
    "ncpi0_scale_factor = 1\n",
    "ccnue_scale_factor = 1\n",
    "\n",
    "for p in unique_pot:\n",
    "    if round(p) == 22:\n",
    "        rockbox_scale_factor = p\n",
    "    if round(p) == 4:\n",
    "        ncpi0_scale_factor = p\n",
    "    if round(p) == 1:\n",
    "        ccnue_scale_factor = p\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"POT scale factor in the array = {}\".format(unique_pot))\n",
    "print(\"Rockbox = {}\".format(rockbox_scale_factor))\n",
    "print(\"NCPi0 = {}\".format(ncpi0_scale_factor))\n",
    "print(\"CCnue = {}\".format(ccnue_scale_factor))\n",
    "        \n",
    "df_nu_rockbox = df_nu[df_nu['scale_pot'] ==  rockbox_scale_factor]\n",
    "df_nu_ncpi0 = df_nu[df_nu['scale_pot'] ==  ncpi0_scale_factor]\n",
    "df_nu_ccnue = df_nu[df_nu['scale_pot'] ==  ccnue_scale_factor]\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"total nu #events = {}\".format(len(df_nu)))\n",
    "print(\"rockbox #events = {}\".format(len(df_nu_rockbox)))\n",
    "print(\"ncpi0 #events = {}\".format(len(df_nu_ncpi0)))\n",
    "print(\"ccnue #events = {}\".format(len(df_nu_ccnue)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Organise Some Stuff</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnl_dict = {}\n",
    "hnl_cos_dict = {}\n",
    "\n",
    "rockbox_dict = {}\n",
    "ncpi0_dict = {}\n",
    "ccnue_dict = {}\n",
    "\n",
    "cos_dict = {}\n",
    "\n",
    "cos_error_list = ['stat']\n",
    "hnl_error_list = ['stat', 'flx', 'mistagging']\n",
    "nu_error_list = ['stat', 'flx', 'xsec', 'g4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnl_label = str(m) + ' MeV HNL ${\\pi}^{0}$' + '\\n' + '|U$_{{\\mu 4}}$|$^{{2}}$ = ' +str(sci_notation(fitU,0,0))\n",
    "nu_label = \"Neutrinos\"\n",
    "rockbox_label = \"Rockbox Neutrinos\"\n",
    "ncpi0_label = r\"NC$\\pi^{0}$\"\n",
    "ccnue_label = r\"CC$\\nu_{e}$\"\n",
    "cos_label = \"Cosmics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plot Individual Sample</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1, figsize = (6,4))\n",
    "\n",
    "xmin, xmax, xnbin = xmin, xmax, xnbin\n",
    "xlimmin, xlimmax = xmin, xmax\n",
    "print(bins)\n",
    "#-----------------------------------------------------------------#\n",
    "pltdf = df_hnl['mod_t']\n",
    "weights = [hnl_scale_factor]*len(df_hnl['mod_t']) \n",
    "\n",
    "_, _, _ = ax1.hist(\n",
    "                            pltdf,\n",
    "                            bins = bins, \n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = hnl_col,\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = hnl_label\n",
    "                        )\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "ax1.legend(loc = 'upper left',fontsize = 12)\n",
    "plot_tick(ax1, 16)\n",
    "plot_title(ax1, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  r\"Slices (1$\\times10^{21}$ POT)\", 16)\n",
    "\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.set_ylim(1, 100)\n",
    "#-----------------------------------------------------------------#\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_cv.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (18,4))\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "pltdf = df_nu_rockbox['mod_t']\n",
    "weights = df_nu_rockbox['scale_pot']\n",
    "_, _, _ = ax1.hist(\n",
    "                            pltdf,\n",
    "                            bins = bins, \n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = nu_col,\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = rockbox_label\n",
    "                        )\n",
    "#-----------------------------------------------------------------#\n",
    "pltdf = df_nu_ncpi0['mod_t']\n",
    "weights = df_nu_ncpi0['scale_pot']\n",
    "_, _, _ = ax2.hist(\n",
    "                            pltdf,\n",
    "                            bins = bins, \n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = nu_col,\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = ncpi0_label\n",
    "                        )\n",
    "#-----------------------------------------------------------------#\n",
    "pltdf = df_nu_ccnue['mod_t']\n",
    "weights = df_nu_ccnue['scale_pot']\n",
    "_, _, _ = ax3.hist(\n",
    "                            pltdf,\n",
    "                            bins = bins, \n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = nu_col,\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = ccnue_label\n",
    "                        )\n",
    "#-----------------------------------------------------------------#\n",
    "ax1.legend(loc = 'upper left',fontsize = 12)\n",
    "plot_tick(ax1, 16)\n",
    "plot_title(ax1, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  r\"Slices (1$\\times10^{21}$ POT)\", 16)\n",
    "\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.set_ylim(1, 600)\n",
    "#-----------------------------------------------------------------#\n",
    "ax2.legend(loc = 'upper left',fontsize = 12)\n",
    "plot_tick(ax2, 16)\n",
    "plot_title(ax2, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  r\"Slices (1$\\times10^{21}$ POT)\", 16)\n",
    "\n",
    "ax2.set_xlim(xmin, xmax)\n",
    "ax2.set_ylim(1, 1800)\n",
    "#-----------------------------------------------------------------#\n",
    "ax3.legend(loc = 'upper left',fontsize = 12)\n",
    "plot_tick(ax3, 16)\n",
    "plot_title(ax3, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  r\"Slices (1$\\times10^{21}$ POT)\", 16)\n",
    "\n",
    "ax3.set_xlim(xmin, xmax)\n",
    "ax3.set_ylim(1, 200)\n",
    "#-----------------------------------------------------------------#\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(savePath+str(\"neutrino_cv.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (6,4))\n",
    "\n",
    "#-----------------------------------------------------------------#\n",
    "ax.step(bins, np.insert(cos_flat_rebin*rockbox_scale_factor, 0, 0)\n",
    "         , color = cos_col\n",
    "         , label =  \"Cosmics\"\n",
    ")\n",
    "#-----------------------------------------------------------------#\n",
    "ax.legend(loc = 'upper left',fontsize = 14)\n",
    "\n",
    "plot_tick(ax, 16)\n",
    "plot_title(ax, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  r\"Slices (1$\\times10^{21}$ POT)\", 16)\n",
    "\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(0, 2 * rockbox_scale_factor)\n",
    "#-----------------------------------------------------------------#\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(savePath+str(\"cosmics_cv.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plot Overlay Sample</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1, figsize = (6,4))\n",
    "\n",
    "xmin, xmax, xnbin = xmin, xmax, xnbin\n",
    "xlimmin, xlimmax = xmin, xmax\n",
    "#-----------------------------------------------------------------#\n",
    "\n",
    "pltdf = df_nu['mod_t']\n",
    "weights = df_nu['scale_pot']\n",
    "_, _, _ = ax1.hist(\n",
    "                            pltdf,\n",
    "                            bins = bins, \n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = nu_col,\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = nu_label\n",
    "                        )\n",
    "#-----------------------------------------------------------------#\n",
    "scale_up_just_once = 10\n",
    "\n",
    "hnl_scale_up_just_once = hnl_scale_factor * scale_up_just_once\n",
    "U_scale_up_just_once = getUfromScaleFactor(fitU, scale_up_just_once)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Sim to Plot scale by = {}\".format(scale))\n",
    "print(\"Fit U = {}\".format(fitU))\n",
    "print(\"Fit POT scale factor = {}\".format(hnl_scale_factor))\n",
    "\n",
    "this_hnl_label = str(m) + ' MeV HNL ${\\pi}^{0}$' + '\\n' + '|U$_{{\\mu 4}}$|$^{{2}}$ = ' +str(sci_notation(U_scale_up_just_once,0,0))\n",
    "#-----------------------------------------------------------------#\n",
    "pltdf = df_hnl['mod_t']\n",
    "weights = [hnl_scale_factor*10]*len(df_hnl['mod_t']) \n",
    "\n",
    "_, _, _ = ax1.hist(\n",
    "                            pltdf,\n",
    "                            bins = bins, \n",
    "                            weights = weights,\n",
    "                            density = False,\n",
    "                            histtype=\"step\",\n",
    "                            edgecolor = hnl_col,\n",
    "                            linestyle = \"-\",\n",
    "                            linewidth = 2,\n",
    "                            label = this_hnl_label\n",
    "                        )\n",
    "#-----------------------------------------------------------------#\n",
    "ax1.step(bins, np.insert(cos_flat_rebin*rockbox_scale_factor, 0, 0)\n",
    "         , color = cos_col\n",
    "         , label =  \"Cosmics\"\n",
    ")\n",
    "#-----------------------------------------------------------------#\n",
    "\n",
    "plot_tick(ax1, 16)\n",
    "plot_title(ax1, \"\", 'Opt0 Time Corrected Z % 18.936 [ns]',  r\"Slices (1$\\times10^{21}$ POT)\", 16)\n",
    "\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.set_ylim(1, 3000)\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "handles[-1] =extra = Rectangle((0, 0), 0.1, 0.1, fc=\"w\", fill=False, edgecolor=cos_col, linewidth=2.0)\n",
    "\n",
    "ax1.legend(handles, labels, loc = 'upper right',fontsize = 12)\n",
    "#-----------------------------------------------------------------#\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(savePath+str(\"signal_bkg_overlay_cv.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>HNL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin it\n",
    "hnl_cv, _ = np.histogram(np.array(df_hnl['mod_t']), bins = bins)\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(hnl_cv)\n",
    "\n",
    "#Make covariance matrix\n",
    "hnl_stat_cov = np.diag(hnl_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "hnl_stat_err = np.sqrt(np.diag(hnl_stat_cov))\n",
    "\n",
    "print(\"\\n stat err\")\n",
    "print(hnl_stat_err)\n",
    "\n",
    "#save in dictionary\n",
    "hnl_cv_plot = np.insert(hnl_cv, 0, 0)\n",
    "\n",
    "hnl_dict['cv'] = hnl_cv\n",
    "hnl_dict['cv_plot'] = hnl_cv_plot\n",
    "\n",
    "hnl_dict['stat_cov'] = hnl_stat_cov\n",
    "hnl_dict['stat_err'] = hnl_stat_err\n",
    "\n",
    "#plot and save\n",
    "plot_hatchy_hatch(hnl_dict, hnl_label, \"hnl\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_statistics_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Flux</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make covariance matrix per variable\n",
    "hnl_flx_cov_array =loopy_loop_multisim_universe(flux_list, flux_name, 1000, df_hnl, hnl_dict, 'hnl', savePath)\n",
    "\n",
    "#Add them together\n",
    "hnl_flx_cov = new_empty_cov()\n",
    "\n",
    "for cov in hnl_flx_cov_array:\n",
    "    hnl_flx_cov = hnl_flx_cov + cov\n",
    "    \n",
    "hnl_flx_err = np.sqrt(np.diag(hnl_flx_cov))\n",
    "\n",
    "#save in dictionary\n",
    "hnl_dict['flx_cov'] = hnl_flx_cov\n",
    "hnl_dict['flx_err'] = hnl_flx_err\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(hnl_dict, hnl_label, \"hnl\", \"flx_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_flux_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cosmics As Mistagging Events</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make covariance matrix\n",
    "hnl_mistagging_cov = np.outer(cos_flat_rebin_hnl, cos_flat_rebin_hnl)\n",
    "hnl_mistagging_err = np.sqrt(np.diag(hnl_mistagging_cov))\n",
    "\n",
    "print(\"\\n Mistagging err\")\n",
    "print(hnl_mistagging_err)\n",
    "\n",
    "#save in dictionary\n",
    "hnl_dict['mistagging_cov'] = hnl_mistagging_cov\n",
    "hnl_dict['mistagging_err'] = hnl_mistagging_err\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(hnl_dict, hnl_label, \"hnl\", \"mistagging_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_mistagging_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combine Errors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(hnl_dict, hnl_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(hnl_dict, \"hnl\", hnl_label, hnl_error_list)\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_beam_bucket_combined_covariance.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale To POT AND Umu Coupling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cov_matrix(hnl_dict, hnl_scale_factor, hnl_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combine_err(hnl_dict, \"hnl\", hnl_label, hnl_error_list\n",
    "                  , ifScale = True , scaleYmax = hnl_scale_factor, suffix = '_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"hnl_beam_bucket_combined_covariance_scaled.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Rockbox Neutrino</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin it\n",
    "rockbox_cv, _ = np.histogram(np.array(df_nu_rockbox['mod_t']), bins = bins)\n",
    "\n",
    "rockbox_dict['cv'] = rockbox_cv\n",
    "rockbox_dict['cv_plot'] =  np.insert(rockbox_cv, 0, 0)\n",
    "\n",
    "rockbox_dict['stat_cov'] = np.diag(rockbox_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "rockbox_dict['stat_err'] = np.sqrt(np.diag(rockbox_dict['stat_cov']))\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(rockbox_dict['cv'] )\n",
    "\n",
    "#===========================================================================#\n",
    "\n",
    "#plot and save\n",
    "plot_hatchy_hatch(rockbox_dict, rockbox_label, \"rockbox\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_stats_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Flux</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_flx_cov_array =loopy_loop_multisim_universe(flux_list, flux_name, 1000\n",
    "                                                    , df_nu_rockbox, rockbox_dict, 'rockbox'\n",
    "                                                    , savePath)\n",
    "\n",
    "#Add them together\n",
    "rockbox_flx_cov = new_empty_cov()\n",
    "\n",
    "for cov in rockbox_flx_cov_array:\n",
    "    rockbox_flx_cov = rockbox_flx_cov + cov\n",
    "    \n",
    "rockbox_flx_err = np.sqrt(np.diag(rockbox_flx_cov))\n",
    "\n",
    "rockbox_dict['flx_cov'] = rockbox_flx_cov\n",
    "rockbox_dict['flx_err'] = rockbox_flx_err\n",
    "\n",
    "#plot it\n",
    "\n",
    "plot_hatchy_hatch(rockbox_dict, rockbox_label, \"rockbox\", \"flx_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_flux_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: UniSim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in unisim_list:\n",
    "    df_nu_rockbox[name] = df_nu_rockbox[name].apply(lambda row: check_unisim(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_unisim_cov_array = loopy_loop_unisim(df_nu_rockbox, rockbox_dict, \"rockbox\", savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sigma</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a random gaussian gun\n",
    "mu, sigma = 0, 1 # mean and standard deviation\n",
    "n_univ = len(df_nu['slc_xsec_multisim_total'][0])\n",
    "\n",
    "random_arr = np.random.normal(mu, sigma, n_univ)\n",
    "len_univ = np.arange(0, n_univ)\n",
    "plt.hist(random_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_multisigma_cov_arr = loopy_loop_multisigma(multisigma_list, multisigma_list, random_arr\n",
    "                                                   , df_nu_rockbox, rockbox_dict, \"rockbox\"\n",
    "                                                   , savePath\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_multisim_cov_array =loopy_loop_multisim_universe(multisim_list, multisim_list, 500\n",
    "                                                         , df_nu_rockbox, rockbox_dict, 'rockbox'\n",
    "                                                         , savePath\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Combined</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rockbox_unisim_cov_array))\n",
    "print(len(rockbox_multisigma_cov_arr))\n",
    "print(len(rockbox_multisim_cov_array))\n",
    "\n",
    "rockbox_xsec_cov_array = rockbox_unisim_cov_array + rockbox_multisigma_cov_arr + rockbox_multisim_cov_array\n",
    "print(len(rockbox_xsec_cov_array))\n",
    "\n",
    "rockbox_xsec_cov = new_empty_cov()\n",
    "\n",
    "for cov in rockbox_xsec_cov_array:\n",
    "    rockbox_xsec_cov = rockbox_xsec_cov + cov\n",
    "    \n",
    "rockbox_xsec_err = np.sqrt(np.diag(rockbox_xsec_cov))\n",
    "\n",
    "rockbox_dict['xsec_cov'] = rockbox_xsec_cov\n",
    "rockbox_dict['xsec_err'] = rockbox_xsec_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hatchy_hatch(rockbox_dict, rockbox_label, \"rockbox\", \"xsec_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_xsec_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Geant4 Re-Interactions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockbox_g4_cov = loopy_loop_multisim_universe(g4_list, g4_name, 1000, df_nu_rockbox, rockbox_dict, \"rockbox\", savePath)\n",
    "rockbox_g4_cov = rockbox_g4_cov[0]\n",
    "\n",
    "rockbox_g4_err = np.sqrt(np.diag(rockbox_g4_cov))\n",
    "\n",
    "#save it\n",
    "rockbox_dict['g4_cov'] = rockbox_g4_cov\n",
    "rockbox_dict['g4_err'] = rockbox_g4_err\n",
    "print(rockbox_g4_err)\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(rockbox_dict, rockbox_label, \"rockbox\", \"g4_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_g4_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Combine Errors </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(rockbox_dict, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(rockbox_dict, \"rockbox\", rockbox_label, nu_error_list)\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_beam_bucket_combined_covariance.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Scale to POT </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rockbox_scale_factor)\n",
    "\n",
    "scale_cov_matrix(rockbox_dict, rockbox_scale_factor, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(rockbox_dict, \"rockbox\", rockbox_label, nu_error_list\n",
    "                  , ifScale = True , scaleYmax = rockbox_scale_factor, suffix ='_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"rockbox_beam_bucket_combined_covariance_scaked.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NCPi0</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin it\n",
    "ncpi0_cv, _ = np.histogram(np.array(df_nu_ncpi0['mod_t']), bins = bins)\n",
    "\n",
    "ncpi0_dict['cv'] = ncpi0_cv\n",
    "ncpi0_dict['cv_plot'] =  np.insert(ncpi0_cv, 0, 0)\n",
    "\n",
    "ncpi0_dict['stat_cov'] = np.diag(ncpi0_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "ncpi0_dict['stat_err'] = np.sqrt(np.diag(ncpi0_dict['stat_cov']))\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(ncpi0_dict['cv'] )\n",
    "\n",
    "#===========================================================================#\n",
    "\n",
    "#plot and save\n",
    "plot_hatchy_hatch(ncpi0_dict, ncpi0_label, \"ncpi0\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_stats_error.png\"), dpi=200)\n",
    "\n",
    "plt.show()                                                                                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Flux</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_flx_cov_array =loopy_loop_multisim_universe(flux_list, flux_name, 1000, df_nu_ncpi0, ncpi0_dict, 'ncpi0', savePath)\n",
    "\n",
    "#add cov matrix\n",
    "ncpi0_flx_cov = new_empty_cov()\n",
    "\n",
    "for cov in ncpi0_flx_cov_array:\n",
    "    ncpi0_flx_cov = ncpi0_flx_cov + cov\n",
    "    \n",
    "ncpi0_flx_err = np.sqrt(np.diag(ncpi0_flx_cov))\n",
    "\n",
    "ncpi0_dict['flx_cov'] = ncpi0_flx_cov\n",
    "ncpi0_dict['flx_err'] = ncpi0_flx_err\n",
    "\n",
    "#plot it\n",
    "\n",
    "plot_hatchy_hatch(ncpi0_dict, ncpi0_label, \"ncpi0\", \"flx_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_flx_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: UniSim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in unisim_list:\n",
    "    \n",
    "    df_nu_ncpi0[name] = df_nu_ncpi0[name].apply(lambda row: check_unisim(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_unisim_cov_array = loopy_loop_unisim(df_nu_ncpi0, ncpi0_dict, \"ncpi0\", savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sigma</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_multisigma_cov_arr = loopy_loop_multisigma(multisigma_list, multisigma_list, random_arr\n",
    "                                                 , df_nu_ncpi0, ncpi0_dict, \"ncpi0\"\n",
    "                                                 , savePath\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_multisim_cov_array =loopy_loop_multisim_universe(multisim_list, multisim_list, 500\n",
    "                                                       , df_nu_ncpi0, ncpi0_dict, 'ncpi0'\n",
    "                                                       , savePath\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Combined</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ncpi0_unisim_cov_array))\n",
    "print(len(ncpi0_multisigma_cov_arr))\n",
    "print(len(ncpi0_multisim_cov_array))\n",
    "\n",
    "ncpi0_xsec_cov_array = ncpi0_unisim_cov_array  + ncpi0_multisigma_cov_arr + ncpi0_multisim_cov_array\n",
    "print(len(ncpi0_xsec_cov_array))\n",
    "\n",
    "ncpi0_xsec_cov = new_empty_cov()\n",
    "\n",
    "for cov in ncpi0_xsec_cov_array:\n",
    "    ncpi0_xsec_cov = ncpi0_xsec_cov + cov\n",
    "    \n",
    "ncpi0_xsec_err = np.sqrt(np.diag(ncpi0_xsec_cov))\n",
    "\n",
    "ncpi0_dict['xsec_cov'] = ncpi0_xsec_cov\n",
    "ncpi0_dict['xsec_err'] = ncpi0_xsec_err\n",
    "\n",
    "print(ncpi0_xsec_err)\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(ncpi0_dict, ncpi0_label, \"ncpi0\", \"xsec_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_xsec_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Geant4 Re-Interactions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpi0_g4_cov = loopy_loop_multisim_universe(g4_list, g4_name, 1000, df_nu_ncpi0, ncpi0_dict, \"ncpi0\", savePath)\n",
    "ncpi0_g4_cov = ncpi0_g4_cov[0]\n",
    "\n",
    "ncpi0_g4_err = np.sqrt(np.diag(ncpi0_g4_cov))\n",
    "\n",
    "ncpi0_dict['g4_cov'] = ncpi0_g4_cov\n",
    "ncpi0_dict['g4_err'] = ncpi0_g4_err\n",
    "print(ncpi0_g4_err)\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(ncpi0_dict, ncpi0_label, \"ncpi0\", \"g4_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_g4_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combine Errors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(ncpi0_dict, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(ncpi0_dict, \"ncpi0\", ncpi0_label, nu_error_list)\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_beam_bucket_combined_covariance.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale to POT </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncpi0_scale_factor)\n",
    "\n",
    "scale_cov_matrix(ncpi0_dict, ncpi0_scale_factor, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(ncpi0_dict, \"ncpi0\", ncpi0_label, nu_error_list\n",
    "                 , ifScale = True, scaleYmax = ncpi0_scale_factor, suffix ='_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"ncpi0_beam_bucket_combined_covariance_scaked.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CCNue</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin it\n",
    "ccnue_cv, _ = np.histogram(np.array(df_nu_ccnue['mod_t']), bins = bins)\n",
    "\n",
    "ccnue_dict['cv'] = ccnue_cv\n",
    "ccnue_dict['cv_plot'] =  np.insert(ccnue_cv, 0, 0)\n",
    "\n",
    "ccnue_dict['stat_cov'] = np.diag(ccnue_cv) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "ccnue_dict['stat_err'] = np.sqrt(np.diag(ccnue_dict['stat_cov']))\n",
    "\n",
    "#No Scale\n",
    "print(\"\\nprescale: entries per bin\")\n",
    "print(ccnue_dict['cv'] )\n",
    "\n",
    "#===========================================================================#\n",
    "\n",
    "#plot and save\n",
    "plot_hatchy_hatch(ccnue_dict, ccnue_label, \"ccnue\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ccnue_stats_error.png\"), dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Flux</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccnue_flx_cov_array =loopy_loop_multisim_universe(flux_list, flux_name, 1000, df_nu_ccnue, ccnue_dict, 'ccnue', savePath)\n",
    "\n",
    "#add cov matrix\n",
    "ccnue_flx_cov = new_empty_cov()\n",
    "\n",
    "for cov in ccnue_flx_cov_array:\n",
    "    ccnue_flx_cov = ccnue_flx_cov + cov\n",
    "    \n",
    "ccnue_flx_err = np.sqrt(np.diag(ccnue_flx_cov))\n",
    "\n",
    "ccnue_dict['flx_cov'] = ccnue_flx_cov\n",
    "ccnue_dict['flx_err'] = ccnue_flx_err\n",
    "\n",
    "#plot it\n",
    "\n",
    "plot_hatchy_hatch(ccnue_dict, ccnue_label, \"ccnue\", \"flx_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ccnue_flx_err.png\"), dpi=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Unisim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in unisim_list:\n",
    "    \n",
    "    df_nu_ccnue[name] = df_nu_ccnue[name].apply(lambda row: check_unisim(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccnue_unisim_cov_array = loopy_loop_unisim(df_nu_ccnue, ccnue_dict, \"ccnue\", savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sigma</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccnue_multisigma_cov_arr = loopy_loop_multisigma(multisigma_list, multisigma_list, random_arr\n",
    "                                                 , df_nu_ccnue, ccnue_dict, \"ccnue\"\n",
    "                                                 , savePath\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Multi-Sim</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccnue_multisim_cov_array =loopy_loop_multisim_universe(multisim_list, multisim_list, 500\n",
    "                                                       , df_nu_ccnue, ccnue_dict, 'ccnue'\n",
    "                                                       , savePath\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Section: Combined</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ccnue_unisim_cov_array))\n",
    "print(len(ccnue_multisigma_cov_arr))\n",
    "print(len(ccnue_multisim_cov_array))\n",
    "\n",
    "ccnue_xsec_cov_array = ccnue_unisim_cov_array  + ccnue_multisigma_cov_arr + ccnue_multisim_cov_array\n",
    "print(len(ccnue_xsec_cov_array))\n",
    "\n",
    "ccnue_xsec_cov = new_empty_cov()\n",
    "\n",
    "for cov in ccnue_xsec_cov_array:\n",
    "    ccnue_xsec_cov = ccnue_xsec_cov + cov\n",
    "    \n",
    "ccnue_xsec_err = np.sqrt(np.diag(ccnue_xsec_cov))\n",
    "\n",
    "ccnue_dict['xsec_cov'] = ccnue_xsec_cov\n",
    "ccnue_dict['xsec_err'] = ccnue_xsec_err\n",
    "\n",
    "print(ccnue_xsec_err)\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(ccnue_dict, ccnue_label, \"ccnue\", \"xsec_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ccnue_xsec_err.png\"), dpi=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Geant4 Re-Interactions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccnue_g4_cov = loopy_loop_multisim_universe(g4_list, g4_name, 1000, df_nu_ccnue, ccnue_dict, \"ccnue\", savePath)\n",
    "ccnue_g4_cov = ccnue_g4_cov[0]\n",
    "\n",
    "ccnue_g4_err = np.sqrt(np.diag(ccnue_g4_cov))\n",
    "\n",
    "ccnue_dict['g4_cov'] = ccnue_g4_cov\n",
    "ccnue_dict['g4_err'] = ccnue_g4_err\n",
    "print(ccnue_g4_err)\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(ccnue_dict, ccnue_label, \"ccnue\", \"g4_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"ccnue_g4_error.png\"), dpi=200)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combine Errors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(ccnue_dict, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(ccnue_dict, \"ccnue\", ccnue_label, nu_error_list)\n",
    "\n",
    "plt.savefig(savePath+str(\"ccnue_beam_bucket_combined_covariance.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale to POT </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ccnue_scale_factor)\n",
    "\n",
    "scale_cov_matrix(ccnue_dict, ccnue_scale_factor, nu_error_list)\n",
    "\n",
    "#plot it\n",
    "plot_combine_err(ccnue_dict, \"ccnue\", ccnue_label, nu_error_list\n",
    "                 , ifScale = True, scaleYmax = ccnue_scale_factor, suffix ='_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"ccnue_beam_bucket_combined_covariance_scaked.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fuse Neutrino Background Into One Please</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_dict_list = [rockbox_dict, ncpi0_dict, ccnue_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_dict = {}\n",
    "\n",
    "nu_dict['cv_scale'] = np.array([0]*(len(bins)-1))\n",
    "nu_dict['cv_plot_scale'] = np.array([0]*(len(bins)))\n",
    "\n",
    "for d in nu_dict_list:\n",
    "    nu_dict['cv_scale'] = nu_dict['cv_scale'] + d['cv_scale']\n",
    "    nu_dict['cv_plot_scale'] = nu_dict['cv_plot_scale'] + d['cv_plot_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_nan_plz(rockbox_dict)\n",
    "fill_nan_plz(ncpi0_dict)\n",
    "fill_nan_plz(ccnue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_dict['combined_cov_scale'] = new_empty_cov()\n",
    "\n",
    "for err in nu_error_list:\n",
    "    print(err)\n",
    "    nu_dict[err+'_cov_scale'] = new_empty_cov()\n",
    "    \n",
    "    for d in nu_dict_list:\n",
    "        nu_dict[err+'_cov_scale'] = nu_dict[err+'_cov_scale'] + d[err+'_cov_scale']\n",
    "    \n",
    "    nu_dict[err+'_err_scale'] = np.sqrt(np.diag(nu_dict[err+'_cov_scale']))\n",
    "    \n",
    "    nu_dict[err+'_cov_frac_scale'] = nu_dict[err+'_cov_scale'] / np.outer(nu_dict['cv_scale'], nu_dict['cv_scale'])\n",
    "    nu_dict[err+'_frac_err_scale'] = np.sqrt(np.diag(nu_dict[err+'_cov_frac_scale']))\n",
    "        \n",
    "    nu_dict['combined_cov_scale'] = nu_dict['combined_cov_scale'] + nu_dict[err+'_cov_scale']\n",
    "    \n",
    "nu_dict['combined_err_scale'] = np.sqrt(np.diag( nu_dict['combined_cov_scale']))\n",
    "    \n",
    "nu_dict['combined_cov_frac_scale'] = nu_dict['combined_cov_scale'] / np.outer(nu_dict['cv_scale'], nu_dict['cv_scale'])\n",
    "nu_dict['combined_frac_err_scale'] = np.sqrt(np.diag(nu_dict['combined_cov_frac_scale']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combine_err(nu_dict, \"nu\", \"Neutrinos\", nu_error_list\n",
    "                  , ifScale = True, suffix = '_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"combined_neutrino_beam_bucket_combined_covariance_scaled.png\"), dpi=200)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Cosmics </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cos_flat_rebin)\n",
    "\n",
    "cos_dict['cv'] = cos_flat_rebin\n",
    "cos_dict['cv_plot'] = np.insert(cos_dict['cv'], 0, 0)\n",
    "\n",
    "#This is the smart way\n",
    "cos_stat_cov = np.diag(cos_flat_rebin) #[some NxN covariance matrix e.g. np.diag(cv) for statistical]\n",
    "cos_stat_err = np.sqrt(np.diag(cos_stat_cov))\n",
    "\n",
    "print(\"\\n stat err\")\n",
    "print(cos_stat_err)\n",
    "\n",
    "cos_dict['stat_cov'] = cos_stat_cov\n",
    "cos_dict['stat_err'] = cos_stat_err\n",
    "\n",
    "#plot it\n",
    "plot_hatchy_hatch(cos_dict, \"Cosmics\", \"cos\", \"stat_err\")\n",
    "\n",
    "plt.savefig(savePath+str(\"cosmic_stat_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale to POT to Rockbox Number </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_error(cos_dict, cos_error_list)\n",
    "\n",
    "scale_cov_matrix(cos_dict, rockbox_scale_factor, cos_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combine_err(cos_dict, \"cos\", cos_label, cos_error_list, ifScale = True, scaleYmax = 25, suffix = '_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"combined_cosmics_err_scaled.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fuse Neutrino and Cosmics Into A Single Background</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_nan_plz(cos_dict)\n",
    "fill_nan_plz(nu_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_dict = {}\n",
    "\n",
    "#combine cv\n",
    "bkg_dict['cv_scale'] = nu_dict['cv_scale'] + cos_dict['cv_scale'] \n",
    "bkg_dict['cv_plot_scale'] = nu_dict['cv_plot_scale'] + cos_dict['cv_plot_scale'] \n",
    "\n",
    "#combine stat errors\n",
    "bkg_dict['stat_cov_scale'] = nu_dict['stat_cov_scale'] + cos_dict['stat_cov_scale']\n",
    "\n",
    "bkg_dict['stat_err_scale'] = np.sqrt(np.diag(bkg_dict['stat_cov_scale']))\n",
    "\n",
    "bkg_dict['stat_cov_frac_scale'] = bkg_dict['stat_cov_scale'] / np.outer(bkg_dict['cv_scale'], bkg_dict['cv_scale'])\n",
    "bkg_dict['stat_frac_err_scale'] = np.sqrt(np.diag(bkg_dict['stat_cov_frac_scale']))\n",
    "\n",
    "#flux / xsec / g4 are taken directly from neutrino samples\n",
    "\n",
    "bkg_dict['flx_err_scale'] = nu_dict['flx_frac_err_scale']\n",
    "bkg_dict['xsec_err_scale'] = nu_dict['xsec_frac_err_scale']\n",
    "bkg_dict['g4_err_scale'] = nu_dict['g4_frac_err_scale']\n",
    "\n",
    "bkg_dict['flx_frac_err_scale'] = nu_dict['flx_frac_err_scale']\n",
    "bkg_dict['xsec_frac_err_scale'] = nu_dict['xsec_frac_err_scale']\n",
    "bkg_dict['g4_frac_err_scale'] = nu_dict['g4_frac_err_scale']\n",
    "\n",
    "#quadrature sum each uncertainty\n",
    "bkg_dict['combined_err_scale'] = np.array([0]*(len(bins)-1))\n",
    "bkg_dict['combined_frac_err_scale'] = np.array([0]*(len(bins)-1))\n",
    "\n",
    "nu_error_list = ['stat','flx','xsec', 'g4']\n",
    "\n",
    "for err in nu_error_list:\n",
    "    print(err)\n",
    "    bkg_dict['combined_err_scale'] = bkg_dict['combined_err_scale'] + bkg_dict[err+'_err_scale']**2\n",
    "    bkg_dict['combined_frac_err_scale'] = bkg_dict['combined_frac_err_scale'] + + bkg_dict[err+'_frac_err_scale']**2\n",
    "    \n",
    "bkg_dict['combined_err_scale'] = np.sqrt(bkg_dict['combined_err_scale'])\n",
    "bkg_dict['combined_frac_err_scale'] = np.sqrt(bkg_dict['combined_frac_err_scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combine_err(bkg_dict, \"nu\", \"Neutrino + Cosmic Background\"\n",
    "                 , nu_error_list\n",
    "                 , ifScale = True, scaleYmax = 1.2, suffix = '_scale')\n",
    "\n",
    "plt.savefig(savePath+str(\"full_background_err.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save HNL </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnl_dict_save = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnl_dict_save['U'] = fitU\n",
    "\n",
    "hnl_dict_save['m'] = m\n",
    "\n",
    "hnl_dict_save['signal'] = hnl_dict['cv_scale'].tolist()\n",
    "\n",
    "hnl_dict_save['stat_err'] = hnl_dict['stat_err_scale'].tolist()\n",
    "\n",
    "hnl_dict_save['flx_err'] = hnl_dict['flx_err_scale'].tolist()\n",
    "\n",
    "hnl_dict_save['mistagging_err'] = hnl_dict['mistagging_err_scale'].tolist()\n",
    "\n",
    "hnl_dict_save['combined_err'] = hnl_dict['combined_err_scale'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in zip(hnl_dict_save.keys(), hnl_dict_save.values()):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../pkl_files/v3_April2024/hnl_m\"+str(m)+\"_v3_dict.npy\", hnl_dict_save) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save Neutrinos + Cosmics Background </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_save_dict = {}\n",
    "\n",
    "bkg_save_dict['bkg'] = bkg_dict['cv_scale'].tolist()\n",
    "\n",
    "bkg_save_dict['stat_err'] = bkg_dict['stat_err_scale'].tolist()\n",
    "\n",
    "bkg_save_dict['flx_err'] = bkg_dict['flx_err_scale'].tolist()\n",
    "\n",
    "bkg_save_dict['xsec_err'] = bkg_dict['xsec_err_scale'].tolist()\n",
    "\n",
    "bkg_save_dict['g4_err'] = bkg_dict['g4_err_scale'].tolist()\n",
    "\n",
    "bkg_save_dict['combined_err'] = bkg_dict['combined_err_scale'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in zip(bkg_save_dict.keys(), bkg_save_dict.values()):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../pkl_files/v3_April2024/bkg_v3_dict.npy\", bkg_save_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save Neutrinos Background </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_save_dict = {}\n",
    "\n",
    "nu_save_dict['bkg'] = nu_dict['cv_scale'].tolist()\n",
    "\n",
    "nu_save_dict['stat_err'] = nu_dict['stat_err_scale'].tolist()\n",
    "\n",
    "nu_save_dict['flx_err'] = nu_dict['flx_err_scale'].tolist()\n",
    "\n",
    "nu_save_dict['xsec_err'] = nu_dict['xsec_err_scale'].tolist()\n",
    "\n",
    "nu_save_dict['g4_err'] = nu_dict['g4_err_scale'].tolist()\n",
    "\n",
    "nu_save_dict['combined_err'] = nu_dict['combined_err_scale'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in zip(bkg_save_dict.keys(), bkg_save_dict.values()):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../pkl_files/v3_April2024/nu_v3_dict.npy\", nu_save_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save Rockbox Cosmics Background </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_save_dict = {}\n",
    "\n",
    "cos_save_dict['bkg'] = cos_dict['cv_scale'].tolist()\n",
    "\n",
    "cos_save_dict['stat_err'] = cos_dict['stat_err_scale'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in zip(cos_save_dict.keys(), cos_save_dict.values()):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../pkl_files/v3_April2024/cos_v3_dict.npy\", cos_save_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
